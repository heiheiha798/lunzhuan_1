# 第一轮科研轮转报告（李萌）

<div style="text-align:right; font-family:'SimSun'; font-size:10.5pt;">
    杨天健、汤子瑜
</div>

## 研究成果报告

### 课题背景（第一周）
选定老师后的周日，李萌老师简单介绍了课题组研究方向

<div style="display: flex; justify-content: center; align-items: center;">
   <img src="E:\BaiduSyncdisk\25-1\科研轮转\LM\topic\part1.png" alt="part1" style="zoom: 19%;margin-right: 10px;" />
   <img src="E:\BaiduSyncdisk\25-1\科研轮转\LM\topic\part2.png" alt="part2" style="zoom:19%;" />
</div>

经过一些考虑，我们决定组队，选择“KV Cache compression on a heterogeneous CPU/GPU platform”作为轮转课题，被分配一名博二学长指导开展研究。

### 第一阶段：基础知识学习（第二周）
1. 深度学习理论基础（1天）
   - 速通《苹果书》约100页阅读（深度学习基础、CNN、自注意力、Transformer、自监督学习等）
   - 观看“跟李沐学AI”约7小时的b站视频（Alexnet、Resnet、Transformer、Bert、GPT、llama论文精读）
   - 阅读一些LLM中Attention算法相关论文（Squeezed Attention、Flash Attention等）
2. 大模型代码学习（6天）
   - 找学长开通服务器账号，学习基本的anaconda环境配置
   - 在学长的指导下，开始自学qwen2（阿里的千问2大模型）代码，这里参考了很多博客
     1）`https://blog.csdn.net/victor_manches/article/details/143775696 `
     2）`https://blog.csdn.net/DEVELOPERAA/article/details/140144973`
     3）`https://www.cnblogs.com/obullxl/p/18244805/NTopic2024061201`
     4）`https://blog.csdn.net/datian1234/article/details/144716078`
     具体的学习过程可见：`https://github.com/heiheiha798/lunzhuan_1`
   - 学习完 qwen2后，在学长的指示下，下载最新的 KTransformers，
     详见`https://github.com/kvcache-ai/ktransformers`，途中出现了很多环境配置的报错，参考了清华自己的直播`https://www.bilibili.com/video/BV18rQWY9E8L/`，但仍然存在错误，最终在学长帮助下解决。代码学习详见`https://github.com/heiheiha798/lunzhuan_1`

这里插播学习总结，大致参考《Challenges in Deploying Long-Context Transformers: A Theoretical Peak Performance Analysis》，因为之后都是代码debug的工作，比较枯燥。

#### Transformer架构

<div style="display: flex; justify-content: center; align-items: center;">
  <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250409230319931.png" alt="image-20250409230319931" style="zoom:25%;" />
</div>

Transformer的核心是自注意力机制，其计算过程涉及三个关键矩阵：  

- **Query (Q)**: 表示当前token的查询向量  
- **Key (K)/Value (V)**: 表示历史token的键值对  
- 注意力得分为：$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$  

在自回归生成中，每个新token会与所有历史token计算注意力，导致计算复杂度随序列长度呈$O(n^2)$增长。  

#### KV Cache的引入与作用  

为优化推理效率，实际部署时会缓存历史token的$K$/$V$矩阵（即**KV Cache**）：  

- 存储形式：每层每头维护独立的$K$/$V$张量，形状为$[seq\_len, hidden\_dim]$  
- 内存占用：对于$L$层$H$头模型，缓存长度为$N$的序列需占用$2 \times L \times H \times N \times d_{head}$内存  

**典型案例**：  

- 34B模型（L=48, H=64, $d_{head}=128$）处理50K上下文时，KV Cache需占用**24GB显存**（约为A100 HBM容量的30%）  

#### KV Cache优化的必要性  

可归纳四大核心问题：  

| 挑战类型       | 表现示例                    | 根本原因              |
| -------------- | --------------------------- | --------------------- |
| **预填充延迟** | 50K上下文预填充耗时>10秒    | $O(n^2)$注意力计算    |
| **并发限制**   | A100仅支持3个50K并发请求    | HBM容量被KV Cache占满 |
| **解码延迟**   | 每token解码延迟增加3倍      | KV Cache数据搬运开销  |
| **切换开销**   | HBM→DDR交换导致停顿达毫秒级 | KV Cache溢出          |

#### 硬件进步的局限性

- **算力-存储墙**：硬件算力增长（如H100→B100）无法匹配KV Cache的$O(N^2)$计算/$O(N)$存储需求（论文核心论断："硬件无法弥合4K与50K的成本差距"）。
- **带宽瓶颈**：KV Cache频繁访问显存，受限于内存带宽（如HBM3带宽仅3TB/s，难以满足长上下文需求）。

#### 算法优化的核心方向

| 维度       | 压缩潜力 | 典型方法                     | 论文依据                 |
| ---------- | -------- | ---------------------------- | ------------------------ |
| **Head**   | ★★★★★    | GQA、稀疏注意力（如20/1024） | "注意力头具有极高稀疏性" |
| **Layer**  | ★★★★     | LayerSkip、动态深度          | "深层对长程依赖贡献低"   |
| **Token**  | ★★       | Token合并/剪枝（如SnapKV）   | "Token压缩比难超50%"     |
| **Hidden** | ★        | 量化（如KIVI-2bit）          | "隐藏维度压缩研究空白"   |

#### 分场景优化策略

- 长上下文推理（>200K）：  
  聚焦预填充优化（如线性注意力、Chunkwise计算），论文指出"预填充延迟占主导"。
- 多轮对话：  
  优化解码效率（推测解码、KV Cache共享），需减少重复计算（论文提及"小模型解码更关键"）。

### 第二阶段：代码移植工作（第三周）
- 部署HybriMoE: 这里首先下载了公开版本的HybriMoE，但是这一版有bug，请教学长后学长拉我们进私有库，顺利下载安装了HybriMoE，随后找学长大致了解到要用Nsight System来看代码运行过程中的资源占用和调度情况，发现代码的一些问题，具体体现在CPU的占用极高，GPU几乎没有占用，问学长无果，因为“这个代码不是我写的”（学长原话），最终非常艰难地发现了原因所在（其实是一个很简单的错误，感受到了学长的幽默）。
- 接下来第二阶段的任务是将HybriMoE移植到新版KTransformers，这里又发现HybriMoE的代码存在bug（学长仍无法解决），经过艰难的debug，最终成功做出了移植后的HybriMoE，代码可以在https://github.com/shuzhangzhong/HybriMoE-Release中查看，这部分的贡献基本由杨天健完成，由于代码库在未来的相当一段时间内是private状态，具体commit见下，可以看到`1,176 additions and 293 deletions`。值得一提的是，这一部分实际上工作量很高，因为这个代码对内存的占用极高，无法使用IDE的debug，基本代码运行一次就要5min，体验非常糟糕。

<div style="display: flex; justify-content: center; align-items: center;">
  <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250409232055734.png" alt="image-20250409232055734" style="zoom: 25%;" />
</div>

- 移植完成后再次运行Nsys，得到了一个较为理想的Overlap图像，到这里其实科研轮转的内容已经差不多结束了

<div style="display: flex; justify-content: center; align-items: center;">
  <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250323131656411 - 副本.png" alt="image-20250323131656411 - 副本" style="zoom:20%;" />
</div>

### 第三阶段：文献调研（第四周）
- 完成代码的移植后，学长指示开始看一些KV Cache的论文，学长友情提供了一篇FastDecode然后吃了一周的麦当劳，其余的论文主要靠杨天健用爬虫爬arXiv得到，两人总共大致阅读了30篇论文，并做了较为详细的笔记，详见第二部分。

### 第四阶段：迷茫中（第五周……）

- 李萌老师找我们聊了一下进度，讨论了一下，但是没有什么新的idea，周末又开了一个小会，仍然没有什么可执行的idea，老师和学长都无法给出明确的指导之后应该做什么，但是说“如果两周能做出个demo，可以尝试投会议”，我们感到了老师的幽默。和第一届信班的学长聊了一下，了解到了组里其实没人做这个课题，只有科研轮转在做，我们感到了科研轮转的幽默。
- 第六周开始就是期中周，我们主要中心转移到期中考试，没有尝试做出“demo”。

这一部分除了单独提及的部分基本由两人共同完成。很多代码学习的细节无法给出，具体见

  `https://github.com/heiheiha798/lunzhuan_1`







## 文献阅读报告


### DéjàVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving

#### Abstract

分布式 LLM 服务成本高昂，并且由于三个关键挑战而经常无法充分利用硬件加速器：由于提示和令牌处理的双峰延迟导致的管道并行部署中的气泡、GPU 内存过度配置以及发生故障时的长恢复时间。在本文中，我们提出了 DéjàVu，这是一个使用多功能高效 KV 缓存流库 (DéjàVuLib) 解决所有这些挑战的系统。使用 DéjàVuLib，我们提出并实现了高效的提示令牌分解以减少管道气泡、微批交换以实现高效的 GPU 内存管理以及状态复制以实现容错。我们重点介绍了这些解决方案在云部署中的一系列大型模型上的有效性。

#### Solution

首先，为了减少流水线泡沫，我们建议将提示处理与令牌生成分解开来，为每项任务分配独立的机器。通过避免将提示任务和标记任务混合在一起，分解有助于减少流水线泡沫，提高吞吐量。然而，分解的有效性依赖于 promptKV 缓存的快速传输，这可能会成为一个瓶颈，尤其是因为用户提交的提示会越来越大，这就更加凸显了高效 K V 缓存流机制的必要性。

其次，为了优化 GPU 内存使用，我们建议在微批次级别在 GPU 和 CPU 之间交换 KV 缓存。所有正在进行的微批次的 KV 缓存都存储在 CPU 中，并且仅在处理相应的微批次时才传输到 GPU。这大大降低了 GPU 内存需求，支持更大的批次大小，并便于在有限的硬件下提供 LLM 服务。但是，通过带宽有限的 PCle 进行的 CPU-GPU 传输可能成为瓶颈，可能会导致 GPU 闲置。

<div style="display: flex; justify-content: center; align-items: center;">
   <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250330143028932.png" alt="image-20250330143028932" style="zoom:33%;" />
</div>

#### Method

**Buffered copies** : 

我们利用 GPUDRAM 的高带宽，并将所有更新聚合到 GPU 内存中的临时缓冲区中。填充临时缓冲区后，我们将其复制到适当的目的地。由于这些缓冲区被重复使用，因此 GPU 内存容量的开销可以忽略不计。

<div style="display: flex; justify-content: center; align-items: center;">
   <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250330144730954.png" alt="image-20250330144730954" style="zoom:33%;" />
</div>

**Layer-by-layer prompt cache streaming** :

由于提示处理以逐层方式进行，因此我们也逐层流式传输提示缓存。这类似于无等待反向传播，它将反向传递计算与分布式 ML 训练中的梯度交换重叠。在流水线并行设置中，我们进一步并行化微批次 i 的提示流式传输与微批次 i + 1 的计算。

<div style="display: flex; justify-content: center; align-items: center;">
   <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250330144909614.png" alt="image-20250330144909614" style="zoom:33%;" />
</div>

**Token computation and streaming parallelization**:

与即时处理不同，单个请求的令牌生成涉及多个步骤。在单机设置中，我们在步骤 $i + 1$ 正在进行时流式传输步骤 $i$ 的 KV 缓存。在流水线并行设置中，我们将微批次$ i$、步骤 $j$ 的缓存流与微批次$ i+ 1$、步骤 $j$ 的计算并行化。令牌计算与即时处理一样，是逐层进行的。但是，令牌流时间可以完全隐藏在后续令牌计算之后，因此在这种情况下我们不使用逐层流。我们使用负责缓存流的后台 CPU 线程和 CUDA 流来并行化 KV 缓存流与 GPU 上的计算。

<div style="display: flex; justify-content: center; align-items: center;">
   <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250330145144113.png" alt="image-20250330145144113" style="zoom: 40%;" />
</div>

#### Detailed description

**PROMPT-TOKEN DISAGGREGATION**

工作器分为两类：即时处理和token生成。控制器将传入的请求分配给即时工作器，即时工作器生成第一个token，填充 KV 缓存，然后将其传输到token生成机器。通过分解，我们需要确保：1）我们为每个阶段分配最佳资源，2）我们以最小的开销将 KV 缓存从即时机器传输到token机器。

以下省略一些非常复杂的推导。

该系统通过流水线快速传输 KV 缓存和 GPU 与 CPU 之间的微批次交换来优化分布式推理，以最大限度地提高内存效率。在生成过程中，KV 缓存逐层异步传输，微批次以重叠方式预取到 GPU 并逐出到 CPU。为了实现容错，工作器将 KV 缓存复制到相邻阶段，而中央控制器则监控心跳并处理故障。如果工作器崩溃，恢复包括从副本中恢复丢失的缓存，确定最后一个一致的生成步骤，并从该点重新启动所有工作器以确保正确性。这种方法在保持可靠性的同时，平衡了高吞吐量和低开销。

<div style="display: flex; justify-content: center; align-items: center;">
   <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250330151201179.png" alt="image-20250330151201179" style="zoom: 40%;" />
</div>

#### Performance

<div style="display: flex; justify-content: center; align-items: center;">
  <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250330151322359.png" alt="image1" style="zoom:33%; margin-right: 10px;" />
  <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250330151339595.png" alt="image-20250330151339595" style="zoom:33%;" />
</div>



### FastDecode: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines

#### Abstract

服务大型语言模型 (LLM) 的成本很高，但昂贵且稀缺的 GPU 在按顺序生成 token 时效率很低，除非扩大序列批次。然而，批次大小受到一些不断重用的中间结果（即 KV-Cache）的限制。它们占用太多内存，无法同时将更多序列放入 GPU 中。虽然它们可以卸载到主机内存，但 CPU-GPU 带宽是一个不可避免的瓶颈。我们找到了一种方法，将 Transformer 模型分解为具有不同特征的两个部分，其中一个部分包括内存绑定的 KV-Cache 访问。我们的主要见解是，跨多个节点的 CPU 的聚合内存容量、带宽和计算能力是处理这一部分的有效选择。性能改进来自减少数据传输开销和提高 GPU 吞吐量以处理另一个模型部分。此外，我们使用调度和性能建模技术解决了时间和设备间范围的异构性带来的效率挑战。评估结果表明，在使用相同 GPU 为现代 LLM 提供服务时，我们的系统实现了 vLLM 吞吐量的 1.88 倍 - 5.04 倍。

#### Introduction

我们找到了一种方法，将 Transformer 模型划分为两个部分，即 R 部分和 S 部分。KV-cache 包含在前者中。我们不会通过任何设备间连接传输 KV-cache 数据，而是传输激活张量，激活张量比 KV-cache 小几个数量级。

KV-cache 从 GPU 内存中移除，批处理大小可以大到 1024 或更大。S-Part 中的计算能够以更高的效率利用 GPU。整体 token 生成吞吐量显著增加。这种设计直观地提出了两个问题。（1）CPU 的速度可能不够快，无法匹配 GPU 的吞吐量。（2）在设备之间传输 S-Part 和 R-Part 之间的中间数据可能会很慢。

<div style="display: flex; justify-content: center; align-items: center;">
  <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250330192557833.png" alt="image-20250330192557833" style="zoom:33%;" ; "margin-right: 10px;" />
  <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250330192758407.png" alt="image-20250330192758407" style="zoom:33%;" />
</div>

R-part 完全可以用 CPU, 把 S-Part 放到 GPU, 同时由于传输的是很小的向量 $Q_i,K_i,V_i,O_i$ ，传输延时也很小。

#### Method

##### **System Overview**

<div style="display: flex; justify-content: center; align-items: center;">
  <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250330192937614.png" alt="image-20250330192937614" style="zoom:15%;" />
</div>

##### pipeline

<div style="display: flex; justify-content: center; align-items: center;">
  <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250330193722559.png" alt="image-20250330193722559" style="zoom:17%;" />
</div>

##### **Sequence-level Load-Stabilizing Schedule**

基本的两阶段流水线中存在大量气泡，因为 R-Part 和 S-Part 的工作负载会根据生成序列的长度而发生不同的变化。理想情况下，我们可以移动 R-Part 的工作负载，保持其总量不变。空闲的 S-worker 的三角形区域被“移动”到空闲的 R-worker 的区域。

<div style="display: flex; justify-content: center; align-items: center;">
  <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250330193952434.png" alt="image-20250330193952434" style="zoom:33%;" />
</div>

具体的“移动”方法：

我们在流水线中调度序列，以控制每一步处理的总长度。我们不是同时启动一大批序列，而是以固定间隔 F 步启动较小的微批次。由于目标长度为 S 的序列是在 S 步中生成的，因此每一步都会一起处理多个微批次。同时处理的微批次的总大小大致等于总批次大小。这些微批次的 S 部分作为一个大批次一起处理，因此 GPU 的利用率与以大批次处理序列的方式相同。

##### **Workload-balanced Hardware Selection**

在平衡 latency 前提下尽可能大 Batch ，若 latency 无约束，选择一个 B，进一步增加它只会带来边际吞吐量改进

此外，假设模型的特征维度为 h。S 部分的工作量（反映在 T(B) 中）与 $h^2$ 成正比。同时，R 部分的每个 token 的工作量 R 与 h 成正比。因此，P 大约与 $1/h$ 成正比。h 越大，最佳 CPU 数量往往越小，这通常出现在较大的模型中

#### Performance

<div style="display: flex; justify-content: center; align-items: center;">
  <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250330202453004.png" alt="image1" style="zoom:30%; margin-right: 10px;" />
  <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250330203112350.png" alt="image2" style="zoom:17%;" />
</div>

都是在小模型上的测试，R-part的workload实际上可能非常大，并且Host Memory 可能也不够，还是需要一些eviction，同时CPU需求可能巨高。



### Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving

#### Abstract

Mooncake 是 Kimi 的服务平台，Kimi 是 Moonshot AI 提供的一项领先的 LLM 服务。它采用以 KVCache 为中心的分解式架构，将预填充和解码集群分开。它还利用 GPU 集群中未充分利用的 CPU、DRAM 和 SSD 资源来实现 KVCache 的分解式缓存。Mooncake 的核心是以 KVCache 为中心的调度程序，它在最大化整体有效吞吐量和满足与延迟相关的服务级别目标 (SLO) 之间取得平衡。与假设所有请求都将被处理的传统研究不同，Mooncake 面临着高度超载场景带来的挑战。为了缓解这些问题，我们开发了一种基于预测的早期拒绝策略。实验表明，Mooncake 在长上下文场景中表现出色。与基线方法相比，Mooncake 在某些模拟场景中可以实现高达 525% 的吞吐量提升，同时遵守 SLO。在实际工作负载下，Mooncake 的创新架构使 Kimi 能够处理 75% 以上的请求。

#### Introduction

<div style="display: flex; justify-content: center; align-items: center;">
  <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250401141133288.png" alt="image-20250401141133288" style="zoom:33%;" />
</div>

对于每个请求，全局调度器需要选择一对预填充和解码实例，并按照以下步骤调度请求：1）将尽可能多的可重用 KVCache 转移到选定的预填充实例；2）以块/层的形式完成预填充阶段，并将输出 KVCache 持续流式传输到相应的解码实例；3）加载 KVCache 并将请求添加到解码实例的连续批处理过程中，以生成请求输出。

#### Overview

如图 1 所示，Mooncake 采用分解式架构，不仅将预填充节点与解码节点分离，还将 GPU 集群的 CPU、DRAM、SSD 和 RDMA 资源分组，以实现分解式 KVCache。这种分解式缓存可充分利用未充分利用的资源，提供充足的缓存容量和传输带宽，从而无需额外成本即可实现高效的近 GPU 前缀缓存。

<div style="display: flex; justify-content: center; align-items: center;">
  <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250401163608038.png" alt="image-20250401163608038" style="zoom:33%;" />
</div>

图 3 说明了 KVCache 块的存储和传输逻辑。在 CPU 内存中，KVCache 以分页块的形式存储。根据请求模式，它可以使用缓存驱逐算法，例如 LRU（最近最少使用）、LFU（最不频繁使用）或基于请求特征的算法。这些 KVCache 块在 CPU 和 GPU 之间的传输由一个单独的（GPUDirect）基于 RDMA 的组件 Messenger 处理。该架构还使我们能够向外部用户提供上下文缓存 API，以提高 KVCache 的重用率。

为了调度所有这些分解的组件，Mooncake 在其核心实现了一个名为 Conductor 的全局调度程序。Conductor 负责根据 KVCache 和工作负载的当前分布调度请求。如果有利于未来推理，它还会复制或交换 KVCache 的某些块。具体来说，图 4 演示了请求的典型工作流程。一旦标记完成，指挥者选择一对预填充节点和一个解码节点，并启动包含四个步骤的工作流程：

<div style="display: flex; justify-content: center; align-items: center;">
  <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250401164239959.png" alt="image-20250401164239959" style="zoom:23%;" />
</div>

##### KVCache 重用

选定的预填充节点（组）接收一个请求，该请求包含原始输入、可重用的前缀缓存的块 ID 以及分配给该请求的完整缓存的块 ID。它根据前缀缓存块 ID 将前缀缓存从远程 CPU 内存加载到 GPU 内存中以引导请求。如果不存在前缀缓存，则跳过此步骤。此选择平衡了三个目标：尽可能多地重用 KVCache、平衡不同预填充节点的工作负载以及保证 TTFT SLO。它导致以 KVCache 为中心的调度，这将在第 6 节中进一步讨论。

##### 增量预填充

预填充节点（组）使用前缀缓存完成预填充阶段，并将新生成的增量 KVCache 存储回 CPU 内存。如果未缓存的输入令牌数量超过某个阈值（prefill_chunk），则预填充阶段将拆分为多个块并以流水线方式执行。选择此阈值是为了充分利用相应 GPU 的计算能力，通常大于 1000 个令牌。使用分块但仍分散的预填充节点的原因在 §5.1 中进行了解释。

##### KVCache 传输

每个节点中都部署了上述的 Messenger 服务来管理和传输这些缓存。每个 Messenger 在各自的推理实例中作为独立进程运行，接收信号以实现高速、跨机器的 KVCache 传输。此步骤异步执行，并与上述增量预填充步骤重叠，将每个模型层生成的 KVCache 流式传输到目标解码节点的 CPU 内存中，以减少等待时间。

##### 解码

解码节点的 CPU DRAM 接收到所有 KVCache 后，请求以连续批处理的方式加入下一批次。Conductor 根据其当前负载预先选择解码节点，以确保其不违反 TBT SLO。但是，此 SLO 会由本地调度程序进行双重检查，因为预期负载可能在预填充阶段后发生变化。这种双重检查可能会导致请求被拒绝，在这种情况下，相应的预填充成本就被浪费了。

#### Prefill Pool

**Multi-node Prefill**

Mooncake 利用仅解码器转换器的自回归特性，并实现分块流水线并行 (CPP) 以实现长上下文预填充。我们将预填充集群中的每 X 个节点分组为流水线预填充节点组。对于每个请求，其输入令牌被划分为块，每个块的长度不超过 prefill_chunk。同一请求的不同块可以由不同的节点同时处理，从而实现并行处理并减少 TTFT。

**Layer-wise Prefill**

在 Mooncake 中，KVCache 的加载和存储是通过启动和等待操作异步执行的。在每一层的注意力计算开始之前，模型会等待该层的 KVCache 异步加载完成，并触发下一层 KVCache 的异步加载。在注意力计算完成后，启动该层的 KVCache 的异步存储。一旦所有层的计算完成，该过程将等待所有异步存储操作完成。传输重叠允许预填充实例的执行时间大致相当于 KVCache 加载时间或标准预填充时间，具体取决于前缀缓存相对于输入长度的比例。

#### KVCache-centric Scheduling

**Prefill Global Scheduling**

<div style="display: flex; justify-content: center; align-items: center;">
  <img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20250401184245997.png" alt="image-20250401184245997" style="zoom:33%;" />
</div>

对于每个新请求，其输入令牌被分成几个块，并为每个块计算一个哈希键。这涉及生成块中令牌的哈希键，并与前一个块的哈希键（如果可用）连接。然后将请求的块键与每个预填充实例的缓存键逐一进行比较，以确定前缀匹配长度（prefix_len）。有了这些匹配信息，Conductor 会根据请求长度和 prefix_len（因实例而异）估计相应的执行时间。然后，它会添加该请求的估计等待时间，以获得该实例上的 TTFT。最后，Conductor 将请求分配给具有最短 TTFT 的实例，并相应地更新该实例的缓存和排队时间。如果无法实现 SLO，Conductor 会直接向上层返回 HTTP 429 Too Many Requests 响应状态代码。

**Cache Load Balancing**

解决此 KVCache 调度问题的一个简单解决方案是收集每个块的全局使用情况，使用预测模型预测其未来使用情况，并据此做出调度决策。然而，与预填充时间的估计不同，工作负载是高度动态的，并且会随着时间的推移而发生显着变化。因此，我们提出了一种基于启发式的自动热点迁移方案来增强缓存负载平衡。
